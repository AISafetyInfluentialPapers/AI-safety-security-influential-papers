# AI-safety-security-influential-papers
A selection of influential AI safety and security papers, with up to three papers per topic. Papers are chosen based on their originality and technical quality. We welcome comments on the selection.


## Jailbreak

* Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico Kolter, Matt Fredrikson. “[Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043)”. Arxiv, 2023. 

* Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J. Pappas, Eric Wong. “[Jailbreaking Black Box Large Language Models in Twenty Queries](https://arxiv.org/abs/2310.08419)”. In SaTML, 2025. 

* Yuchen Yang, Bo Hui, Haolin Yuan, Neil Gong, Yinzhi Cao. “[SneakyPrompt: Jailbreaking Text-to-image Generative Models](https://arxiv.org/abs/2305.12082)”. In IEEE Symposium on Security and Privacy, 2024. 


## Watermarking AI-generated content

* Jiren Zhu, Russell Kaplan, Justin Johnson, Li Fei-Fei. “[HiDDeN: Hiding Data With Deep Networks](https://arxiv.org/abs/1807.09937)”. In ECCV, 2018.

* John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, Tom Goldstein. “[A Watermark for Large Language Models](https://arxiv.org/abs/2301.10226)”. In ICML, 2023.

* Robin San Roman, Pierre Fernandez, Alexandre Défossez, Teddy Furon, Tuan Tran, Hady Elsahar. “[Proactive Detection of Voice Cloning with Localized Watermarking](https://arxiv.org/abs/2401.17264)”. In ICML, 2024


## Prompt injection

* Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, Mario Fritz. “[Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection](https://arxiv.org/abs/2302.12173)”. In AISec, 2023. 

* Yupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan Jia, Neil Zhenqiang Gong. “[Formalizing and Benchmarking Prompt Injection Attacks and Defenses](https://arxiv.org/abs/2310.12815)”. In USENIX Security Symposium, 2024.

* Sizhe Chen, Julien Piet, Chawin Sitawarin, David Wagner. “[StruQ: Defending Against Prompt Injection with Structured Queries](https://arxiv.org/abs/2402.06363)”. In USENIX Security Symposium, 2025.


## Adversarial examples

* Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, Rob Fergus. “[Intriguing properties of neural networks](https://arxiv.org/abs/1312.6199)”. In ICLR, 2014. 

* Ian J Goodfellow, Jonathon Shlens, Christian Szegedy. “[Explaining and harnessing adversarial examples](https://arxiv.org/abs/1412.6572)”. In ICLR, 2015.

* Nicholas Carlini, David Wagner.”[Towards evaluating the robustness of neural networks](https://arxiv.org/abs/1608.04644)”. In  IEEE Symposium on Security and Privacy, 2017. 


## Defenses against adversarial examples

* Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu. “[Towards Deep Learning Models Resistant to Adversarial Attacks](https://arxiv.org/abs/1706.06083)”. In ICLR, 2018.

* Eric Wong, Zico Kolter. “[Provable defenses against adversarial examples via the convex outer adversarial polytope](https://arxiv.org/abs/1711.00851)”. In ICML, 2018.

* Jeremy M Cohen, Elan Rosenfeld, J. Zico Kolter. “[Certified Adversarial Robustness via Randomized Smoothing](https://arxiv.org/abs/1902.02918)”. In ICML, 2019. 


## Data poisoning attacks

* Battista Biggio, Blaine Nelson, Pavel Laskov. “[Poisoning Attacks against Support Vector Machines](https://arxiv.org/abs/1206.6389)”. In ICML, 2012.
  
* Ali Shafahi, W. Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, Tom Goldstein. “[​​Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks](https://arxiv.org/abs/1804.00792)”. In NeurIPS, 2018.

## Defenses against data poisoning

* Jacob Steinhardt, Pang Wei Koh, Percy Liang. “[Certified Defenses for Data Poisoning Attacks](https://arxiv.org/abs/1706.03691)”. In NeurIPS, 2017. 

* Alexander Levine, Soheil Feizi. “[Deep Partition Aggregation: Provable Defense against General Poisoning Attacks](https://arxiv.org/abs/2006.14768)”. In ICLR, 2021.
  
* Jinyuan Jia, Xiaoyu Cao, Neil Zhenqiang Gong. “[Intrinsic Certified Robustness of Bagging against Data Poisoning Attacks](https://arxiv.org/abs/2008.04495)”. In AAAI, 2021. 


## Backdoor 

* Tianyu Gu, Brendan Dolan-Gavitt, Siddharth Garg. “[Badnets: Identifying vulnerabilities in the machine learning model supply chain](https://arxiv.org/abs/1708.06733)”. In IEEE Access, 2019.
  
* Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, Dawn Song. “[Targeted backdoor attacks on deep learning systems using data poisoning](https://arxiv.org/abs/1712.05526)”. In arXiv, 2017.
  
* Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, Xiangyu Zhang. “[Trojaning attack on neural networks](https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_03A-5_Liu_paper.pdf)”. In NDSS, 2018. 


## Model poisoning

* Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, Vitaly Shmatikov. “[How To Backdoor Federated Learning](https://arxiv.org/abs/1807.00459)”. In AISTATS, 2020.
  
* Minghong Fang, Xiaoyu Cao, Jinyuan Jia, Neil Zhenqiang Gong. “[Local Model Poisoning Attacks to Byzantine-Robust Federated Learning](https://arxiv.org/abs/1911.11815)”. In Usenix Security Symposium, 2020.



## Model inversion

* Matt Fredrikson, Somesh Jha, Thomas Ristenpart. “[Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/10.1145/2810103.2813677)”. In CCS, 2015.

* Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, Colin Raffel. “[Extracting Training Data from Large Language Models](https://arxiv.org/abs/2012.07805)”. In Usenix Security Symposium, 2021.

* Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramèr, Borja Balle, Daphne Ippolito, Eric Wallace. “[Extracting Training Data from Diffusion Models](https://arxiv.org/abs/2301.13188)”. In Usenix Security Symposium, 2023


## Membership inference

* Reza Shokri, Marco Stronati, Congzheng Song, Vitaly Shmatikov.”[Membership Inference Attacks against Machine Learning Models](https://arxiv.org/abs/1610.05820)”. In IEEE Symposium on Security and Privacy, 2017. 

* Samuel Yeom, Irene Giacomelli, Matt Fredrikson, Somesh Jha. “[Privacy risk in machine learning: Analyzing the connection to overfitting](https://arxiv.org/abs/1709.01604)”. In IEEE Computer Security Foundations Symposium, 2018.

* Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, Florian Tramer. “[Membership Inference Attacks From First Principles](https://arxiv.org/abs/2112.03570)”. In IEEE Symposium on Security and Privacy, 2022.



## Model stealing

* Florian Tramèr, Fan Zhang, Ari Juels, Michael K. Reiter, Thomas Ristenpart. “[Stealing Machine Learning Models via Prediction APIs](https://arxiv.org/abs/1609.02943)”. In Usenix Security Symposium, 2016.

